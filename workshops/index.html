<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=google-site-verification content="MLZcoHabOaOhGgVXj2nNafdpQ0aRkBiCOBUgD6XM6bs"><link rel="shortcut icon" href=/favicon.ico><title>The 20th IEEE International Conference on Automatic Face and Gesture Recognition</title><meta name=description content="The IEEE conference series on Automatic Face and Gesture Recognition is the premier international forum for research in image and video-based face, gesture, and body movement recognition. It is co-sponsored by IEEE Biometrics Council and IEEE Computer Society . Its broad scope includes advances in fundamental computer vision, pattern recognition, and computer graphics; machine learning techniques relevant to face, gesture, and body motion; interdisciplinary research on behavioral analysis; new algorithms and applications. Topics of interest include but are not limited to:
Face recognition & biometrics Face analysis and synthesis Body action and activity recognition Gesture recognition, analysis, and synthesis Affective computing and multi-modal interaction Psychological and behavioral analysis Perceptual and cognitive aspects of non-verbal interaction Databases and tools for FG Technologies and applications related to FG Privacy and ethical issues of FG FG Steering Committee Statement on Broadening ParticipationThe FG community is strongly committed to addressing issues related to diversity, equity, and inclusion at our conference. Diversity has many dimensions, including gender, race, ethnicity, nation of origin, age, disabilities, religious belief, sexual orientation, socioeconomic status, and cultural background. There is no place for hatred and discrimination in the FG community or elsewhere.
For FG 2026, the BP Chairs are working with other members of the organizing committee to help foster a more inclusive, diverse, and equitable conference. Specifically, we will encourage the following:
improved diversity and inclusion in workshop and tutorial organizers and invited speakers by including diversity, equity, and inclusivity as part of the evaluation criteria; the submission of workshop and tutorial proposals that examine problems in equity, diversity, and inclusion from a technical perspective; networking events and support for affinity groups that are specifically aimed at building and nurturing networks in communities that are currently not well represented at FG; support for students to attend FG from communities that do not traditionally attend through travel grants and waived registration fees. FG'26 is committed to supporting students from communities that do not traditionally attend FG through registration and travel support. IEEE Biometrics Council makes a number of grants available for this purpose. Allocation is based on a combination of need, contribution to the conference, where you are traveling from, the community(ies) you identify with, and advisor support.
Important Dates (all AoE ) Main Track : May 25-29, 2026 Round 1 Abstract submission September 25th, 2025 Paper submission October 2nd, 2025 Extended October 6th, 2025 Notifications to authors December 11th, 2025 Round 2 Abstract submission January 9th, 2026 Extended January 25th, 2026 (only for round 2 new submissions) Paper submission January 15th, 2026 Extended January 25th, 2026 Notifications to authors April 2nd, 2026 Camera Ready (for all) April 21st, 2026 Workshops : May 25 or 29, 2026 Proposal deadline November 13th, 2025 November 20th, 2025 Notification of acceptance November 27th, 2025 December 3rd, 2025 Special Sessions : Proposals due November 15th, 2025 November 22, 2025 Notification of acceptance November 30th, 2025 December 7th, 2025 Competitions : Proposal deadline November 21st, 2025 Notification of acceptance November 28th, 2025 December 4th, 2025 Tutorials : May 25 or 29, 2026 Proposal deadline January 13th, 2026 Notification of acceptance January 27th, 2026 Panels : Proposal deadline March 10th, 2026 Notification of acceptance March 20th, 2026 Doctoral Consortium : May 25, 2026 Submission deadline April 9th, 2026 Notification of acceptance April 16th, 2026 Demos : Submission deadline April 9, 2026 (Proposal and Supplemental Material) Notification of acceptance April 16, 2026 Camera Ready Deadline April 21, 2026 "><meta property="og:url" content="/workshops/"><meta property="og:description" content="Workshops Call for Proposals We invite workshop proposals for the 2026 IEEE Conference on Automatic Face and Gesture Recognition (FG 2026) in Kyoto, Japan. Accepted workshops will be held on either May 25 or May 29, 2026, in the same venue as the FG 2026 main conference. Complementary to the main venue, we especially encourage workshop proposals relating to emerging new fields or new application domains of face and gesture analysis and synthesis."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta name=twitter:card content="summary"><meta name=twitter:description content="Workshops Call for Proposals We invite workshop proposals for the 2026 IEEE Conference on Automatic Face and Gesture Recognition (FG 2026) in Kyoto, Japan. Accepted workshops will be held on either May 25 or May 29, 2026, in the same venue as the FG 2026 main conference. Complementary to the main venue, we especially encourage workshop proposals relating to emerging new fields or new application domains of face and gesture analysis and synthesis."><link rel=stylesheet href=/styles.min.62c780de6f20e2bd78368e0dafd1130bf3e6968ed8d6e4e4c554b7ed9f8b4dfa.css type=text/css media=screen integrity="sha256-YseA3m8g4r14No4Nr9ETC/Pmlo7Y1uTkxVS37Z+LTfo="><style>:root{--color-primary-lighter:#154065;--color-primary:#102F4B;--color-primary-darker:#015a57;--color-primary-darkest:#111928;--color-secondary:#f7f8fa;--color-secondary-darker:#637381;--color-track:#154065;--color-shadow:#a6afc366}</style></head><body class=page-body><input type=checkbox class=sidebar-toggle id=sidebarToggle><header class="section-container page-header" id=pageHeader><div class="section section--short page-header__content" id=pageHeaderContent><nav id=mainNavigation class=page-header__navigation aria-labelledby=main-navigation-label><label id=main-navigation-label class=sr-only>Main Menu</label>
<label class=page-header__toggle-sidebar-label for=sidebarToggle><svg width="28" height="16" viewBox="0 0 28 16" fill="none"><rect width="22.4" height="1.33333" fill="#000"/><rect x="5.6001" y="7.33334" width="22.4" height="1.33333" fill="#000"/><rect y="14.6667" width="22.4" height="1.33333" fill="#000"/></svg>
</label><a class=page-header__title-link href=/>FG 2026</a><div class=page-header__menu-container><div id=pageHeaderMenuScriptValidator style=display:none><script>pageHeaderMenuScriptValidator.style.display="inherit"</script><div class=header-menu><a class=header-menu__link href=/>FG 2026</a><div class=dropdown><button class=dropbtn>Authors</button><div class=dropdown__menu><a href=/cfp>Call for Papers
</a><a href=/competition>Competitions
</a><a href=/demos>Demos
</a><a href=/doctoral_consortium>Doctoral Consortium
</a><a href=/special_session>Special Sessions
</a><a href=/tutorials>Tutorials
</a><a href=/workshops>Workshops</a></div></div><a href=/location/ class=header-menu__link>Attend
</a><a href=/organizers/ class=header-menu__link>Organizers
</a><a href=/sponsorship/ class=header-menu__link>Sponsorship</a></div></div></div></nav></div></header><aside class=page-sidebar><nav class=page-sidebar__navigation aria-labelledby=main-navigation-label><header class=page-sidebar__header><a class=page-sidebar__heading-link href=/>FG 2026
</a><label class=page-sidebar__close-label for=sidebarToggle><svg width="17" height="17" viewBox="0 0 17 17" fill="none"><rect x=".943359" width="22.4" height="1.33333" transform="rotate(45 0.943359 0)" fill="#fff"/><rect y="15.8392" width="22.4" height="1.33333" transform="rotate(-45 0 15.8392)" fill="#fff"/></svg></label></header><ul class=sidebar-menu><li class=sidebar-menu__item><a href=/cfp class=sidebar-menu__link aria-label>Call for Papers</a></li><li class=sidebar-menu__item><a href=/competition class=sidebar-menu__link aria-label>Competitions</a></li><li class=sidebar-menu__item><a href=/demos class=sidebar-menu__link aria-label>Demos</a></li><li class=sidebar-menu__item><a href=/doctoral_consortium class=sidebar-menu__link aria-label>Doctoral Consortium</a></li><li class=sidebar-menu__item><a href=/special_session class=sidebar-menu__link aria-label>Special Sessions</a></li><li class=sidebar-menu__item><a href=/tutorials class=sidebar-menu__link aria-label>Tutorials</a></li><li class=sidebar-menu__item><a href=/workshops class=sidebar-menu__link aria-label>Workshops</a></li><li class=sidebar-menu__item></li><a href=/location/ class=sidebar-menu__link aria-label=Attend>Attend</a></li><li class=sidebar-menu__item></li><a href=/organizers/ class=sidebar-menu__link aria-label=Organizers>Organizers</a></li><li class=sidebar-menu__item></li><a href=/sponsorship/ class=sidebar-menu__link aria-label=Sponsorship>Sponsorship</a></li></ul><div class=page-sidebar__social-links><label id=social-links-list-label-0 class=sr-only></label><ul class=social-links aria-labelledby=social-links-list-label-0></ul></div><a class="made-by-medialesson page-sidebar__made-by-link" target=_blank rel="noopener noreferrer external" href=https://gohugo.io>Made with Hugo</a></nav><label class=page-sidebar__empty-space for=sidebarToggle></label></aside><script>const header=document.getElementById("pageHeader"),intercept=document.createElement("div");intercept.setAttribute("data-observer-intercept",""),header.before(intercept);const observer=new IntersectionObserver(([e])=>{header.classList.toggle("page-header--scrolled",!e.isIntersecting)});observer.observe(intercept)</script><script>function checkWhichMenuToUse(){document.body.setAttribute("data-menu-mode","header"),pageHeaderContent.scrollWidth>pageHeaderContent.clientWidth&&document.body.setAttribute("data-menu-mode","sidebar")}new ResizeObserver(checkWhichMenuToUse).observe(document.body)</script><main class=page-main><div class=section-container><section class=section><article class=layout-single-default><h1 class="heading heading--1" id=workshops>Workshops</h1><div class=tabs-container id=workshop-details><input type=radio name=workshop-details id=tab-call-for-proposals-0 class=tab-input>
<label for=tab-call-for-proposals-0 class=tab-label>Call for Proposals</label><div class=tab-content><p>We invite workshop proposals for the 2026 IEEE Conference on Automatic Face and Gesture Recognition (FG 2026) in Kyoto, Japan. Accepted workshops will be held on either May 25 or May 29, 2026, in the same venue as the FG 2026 main conference. Complementary to the main venue, we especially encourage workshop proposals relating to emerging new fields or new application domains of face and gesture analysis and synthesis.</p><h4 class="heading heading--4" id=submission-procedure>Submission Procedure</h4><p>Workshop proposals should include the following information:</p><ul><li>Workshop title</li><li>Workshop motivation, expected outcomes, and impact</li><li>List of organizers including affiliation, email address, and a short bio</li><li>Tentative length of the workshop (half-day or full-day)</li><li>Style of the workshop and related activities, e.g., poster/oral paper presentations, invited talks, round tables, competition, hackathon, etc</li><li>Tentative paper submission and review schedule (Ideally, the camera-ready deadline will coincide with the main conference deadline on April 21)</li><li>Planned advertisement, website hosting, potential sponsorships</li><li>Paper submission procedure (submission website) if applicable</li><li>Paper review procedure (single/double-blind, internal/external,
solicited/invited-only, the pool of reviewers, etc.)</li><li>Tentative program committee, and invited speakers, if any</li><li>Estimated number of submissions and acceptance rate</li></ul><p>Proposals should be submitted through CMT:
<a class=link href=https://cmt3.research.microsoft.com/FG2026 rel=external>https://cmt3.research.microsoft.com/FG2026</a></p><h4 class="heading heading--4" id=contact>Contact</h4><p>For additional information and queries regarding the workshop proposal procedure, please contact the Workshop Co-chairs: <a href=mailto:ukita@toyota-ti.ac.jp>Norimichi Ukita</a> and <a href=mailto:kniinuma@fujitsu.com>Koichiro Niinuma</a>.</p><h4 class="heading heading--4" id=important-dates-all-aoe>Important Dates (all <a class=link href=https://time.is/Anywhere_on_Earth rel=external>AoE</a>
)</h4><table><thead><tr><th style=text-align:left></th><th style=text-align:left></th><th></th></tr></thead><tbody><tr><td style=text-align:left>Workshop proposals due:</td><td style=text-align:left><del>November 13</del>,</td><td style=text-align:left><del>2025</del></td></tr><tr><td style=text-align:left></td><td style=text-align:left><del>November 20</del>,</td><td style=text-align:left><del>2025</del></td></tr><tr><td style=text-align:left>Notification of acceptance:  </td><td style=text-align:left><del>November 27</del>,</td><td style=text-align:left><del>2025</del></td></tr><tr><td style=text-align:left></td><td style=text-align:left><del>December 4</del>,</td><td style=text-align:left><del>2025</del></td></tr><tr><td style=text-align:left>Workshop date:</td><td style=text-align:left>May 25 or 29,  </td><td style=text-align:left>2026</td></tr></tbody></table></div><input type=radio name=workshop-details id=tab-accepted-workshops-1 class=tab-input>
<label for=tab-accepted-workshops-1 class=tab-label>Accepted Workshops</label><div class=tab-content><h4 class="heading heading--4" id=3rd-international-workshop-on-synthetic-data-for-face-and-gesture-analysis-sd-fga><a class=link href=https://sites.google.com/view/ieeefg2026/home rel=external>3rd International Workshop on Synthetic Data for Face and Gesture Analysis (SD-FGA)</a></h4><p>Organizers: Vitomir Struc, Xilin Chen, Fadi Boutros, Naser Damer, Deepak Kumar Jain</p><p>The landscape of computer vision and artificial intelligence is evolving rapidly with the rise of powerful generative models, transforming how data-driven challenges are addressed. Building on the success of the first and second editions of the Synthetic Data for Face and Gesture Analysis (SD-FGA) workshop, SD-FGA 2026 (at FG 2026) will spotlight advances in synthetic data generation-from GANs and VAEs to diffusion models and their growing impact on facial analysis, gesture recognition, and behavioral understanding, where privacy, ethics, and data diversity remain key concerns. This third edition expands the scope to synthetic data for attack generation and detection across both physical and behavioral dimensions, reflecting the urgent need for robust, secure AI in open-world settings. The workshop will bring together researchers and practitioners to examine how synthetic data can enable fair and reliable model development while also supporting the simulation, generation, and detection of adversarial and spoofing attacks, ultimately advancing secure, ethical, and scalable face and gesture analysis systems.</p><h4 class="heading heading--4" id=from-generation-to-authentication-first-workshop-on-trustworthy-face-avatars-trustfa><a class=link href=https://sites.google.com/view/trustfa-fg2026/home rel=external>From Generation to Authentication: First Workshop on Trustworthy Face Avatars (TrustFA)</a></h4><p>Organizers: Ammar Alsherfawi, Allam Shehata, Jianhang Zhou, Yasushi yagi, Bob Zhang, Ruben Tolosana, Luis Gomez, Laura Pedrouzo-Rodriguez, Ruben Vera-Rodriguez</p><p>TrustFA 2026 (From Generation to Authentication: First Workshop on Trustworthy Face Avatars) brings together researchers and practitioners working at the intersection of photo-realistic face avatar and robust authentication to make avatar technologies safer, more reliable, and more accountable. As face reenactment, 2D/3D avatar generation, and real-time facial animation rapidly mature and the raise of new concerns around impersonation, deepfakes, and privacy, this workshop will highlight advances in trustworthy generation, detection and verification, watermarking and traceability, secure capture and control, evaluation protocols and datasets, and emerging applications in telepresence, AR/VR, and human–computer interaction. The program will feature invited talks and contributed presentations, fostering discussion and new collaborations on building face avatar systems people can trust.</p><h4 class="heading heading--4" id=4th-workshop-on-learning-with-few-or-no-annotated-face-body-and-gesture-data><a class=link href=https://sites.google.com/view/lfa-fg2026/ rel=external>4th Workshop on learning with few or no annotated face, body and gesture data</a></h4><p>Organizers: Maxime Devanne, Mohamed Daoudi, Stefano Berretti, Guido Borghi, Germain Forestier, Jonathan Weber</p><p>One of the main limitations of Deep Learning is that it requires large-scale annotated datasets to train efficient models. Gathering face, body or gesture data and annotating them can be very time consuming and laborious. This is particularly the case in areas where experts from the field are required, like in the medical domain. In such a case, using crowdsourcing may not be suitable, also due to privacy concerns and regulations. The goal of this 4th edition of the workshop is to explore approaches to overcome such limitations by investigating ways to learn from few annotated data, to transfer knowledge from similar domains or problems, to generate synthetic data, or to benefit from the community to gather novel large-scale annotated datasets.</p><h4 class="heading heading--4" id=multimodal-foundation-models-for-3d4d-facial-expression-analysis-and-synthesis-mfm-fe-2026><a class=link href=https://sites.google.com/view/mfm-fe rel=external>Multimodal Foundation Models for 3D/4D Facial Expression Analysis and Synthesis (MFM-FE 2026)</a></h4><p>Organizers: Muzammil Behzad, Yante Li, Ajmal Mian, Xiaobai Li, Hui Yu, Zheng Lian</p><p>Facial expression analysis has long been central to understanding human affect, behavior, and communication. However, the emergence of foundation models, spanning vision, language, and multimodal learning, has transformed how subtle and dynamic facial behaviors can be modeled, interpreted, and generated. Traditional CNNor RNN-based approaches, while effective in constrained settings, struggle to generalize across identities, cultures, and real-world variability. In contrast, large-scale pre-trained multimodal architectures offer scalable, transferable, and interpretable representations for 3D/4D facial dynamics, micro- and macro-expression recognition, and textguided expression synthesis. This workshop aims to explore how multimodal and foundation model paradigms can advance facial expression research, thereby moving beyond static emotion recognition to dynamic, context-aware, and linguistically grounded understanding of human affect. It seeks to bring together researchers from affective computing, multimodal learning, behavioral signal processing, and generative modeling to define the next generation of human-centered AI for expressive behavior.</p><h4 class="heading heading--4" id=facial-micro-expression-fme-workshop-2026-pushing-boundaries-in-temporal-and-spatial-subtle-movement-analysis><a class=link href=https://megc2026.github.io/workshop.html rel=external>Facial Micro-Expression (FME) Workshop 2026: Pushing Boundaries in Temporal and Spatial Subtle Movement Analysis</a></h4><p>Organizers: Adrian Davison, Xinqi Fan, Jingting Li, John See, Su-Jing Wang, Moi Hoon Yap</p><p>Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environments. MEs are very short, generally being no more than 500 milliseconds and the data used is often very challenging with the limited number of labelled ME samples. It is also near impossible to unify the standardisation of ME labelling for different annotators. This workshop aims to explore advanced techniques of micro-facial expression analysis using a multimodal approach. We expect new advancements in multimodal micro-expression approaches, using the usual vision and temporal images alongside other metadata. In addition, the rise in large language models and visual language models will further push the boundaries of analysis and overall performance.</p><h4 class="heading heading--4" id=empathic-ai-face-gesture-and-accessibility-technologies-empai-2026><a class=link href=https://empai26.vercel.app/ rel=external>Empathic AI: Face, Gesture, and Accessibility Technologies (EmpAI 2026)</a></h4><p>Organizers: Yutong Zhou, Von Ralph Dane Herbuela, Haifeng Zhang, Nobutaka Shimada, Mariza Ferro</p><p>EmpAI 2026 is the first international workshop dedicated to improving &ldquo;&ldquo;Empathic Intelligence&rdquo;&rdquo;, AI that does more than identification, but understands and assists the diverse emotional, sensory, motor, and cognitive states of all humans. While face, gesture, and multimodal AI have achieved remarkable recognition performance, current systems remain limited when processing non-normative signals from disability, aging, and neurodiversity. We unify empathy, accessibility, and face-gesture research for the first time, aiming to redefine how AI interprets, responds to, and collaborates with diverse users. Bringing together communities across computer vision, HCI/HRI, multimodal LLMs, affective computing, accessibility technologies, and cognitive robotics, this workshop invites researchers, practitioners, and students to establish empathic intelligence as a new research direction, toward AI that not only perceives humans but also connects with, adapts to, and truly supports them.</p><h4 class="heading heading--4" id=1st-workshop-on-behavior-and-emotion-analysis-through-wearable-technology-beat><a class=link href=https://beat-workshop.github.io/ rel=external>1st Workshop on Behavior and Emotion Analysis through wearable Technology (BEAT)</a></h4><p>Organizers: Louis Simon, Arianna De Vecchi, Cristina Palmero, Felix Dollack, Ting Dang, Mohamed Chetouani</p><p>Wearable movement and physiology sensors offer lightweight, non-invasive, and ecologically valid means to monitor human activity, affective state, and social behavior. With the rise of commercially deployed devices and new wearable foundation models, opportunities for scalable human behavior analysis continue to grow. The 1st Workshop on Behavioral and Emotion Analysis through wearable Technology (BEAT) aims to foster collaboration between researchers from various backgrounds (ML, HCI, biomedical engineering) around the topic of wearable devices for human behavioral analysis. Challenges such as resource efficiency, irregularly sampled data, multimodal fusion, and privacy-preserving AI will be addressed. We welcome contributions spanning various application domains, namely Affective Computing, Mobile Health, Action Recognition, Social Interaction, and HRI.</p></div></div><script>(function(){var t,n,s=document.getElementById("workshop-details"),e=s.querySelectorAll(".tab-input");if(e.length>0){for(n=!1,t=0;t<e.length;t++)if(e[t].checked){n=!0;break}n||(e[0].checked=!0)}})()</script></article></section></div></main><footer class=section-container><section class="section section--short page-footer-section"><div class=page-footer-section__start><label id=social-links-list-label-1 class=sr-only>Social media links</label><ul class=social-links aria-labelledby=social-links-list-label-1></ul></div><ul class=footer-menu></ul><div class=page-footer-section__end><a class="made-by-medialesson page-footer-section__made-by-link" target=_blank rel="noopener noreferrer external" href=https://gohugo.io>Made with Hugo</a></div></section></footer></body></html>