<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sessions on</title><link>/sessions/</link><description>Recent content in Sessions on</description><generator>Hugo</generator><language>en</language><atom:link href="/sessions/index.xml" rel="self" type="application/rss+xml"/><item><title>Lunch</title><link>/sessions/lunch-f91513e5-6e73-4910-9ee5-070a6ab04035/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sessions/lunch-f91513e5-6e73-4910-9ee5-070a6ab04035/</guid><description>&lt;p>Fooooood!&lt;/p></description></item><item><title>Scaling 3D Digital Humans</title><link>/sessions/scaling-3d-digital-humans-shunsuke_keynote/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sessions/scaling-3d-digital-humans-shunsuke_keynote/</guid><description>&lt;p>In this talk, we explore the paradigm shift of bringing 3D digital human modeling into the era of large-scale foundation models. While 2D generative models have seen explosive growth, 3D human synthesis has often remained constrained by data scarcity and computational overhead. We bridge this gap by investigating the pre-training and post-training regimes for 3D avatars. We present findings demonstrating that pre-training on massive, diverse datasets—followed by targeted fine-tuning on high-fidelity, curated &amp;ldquo;clean&amp;rdquo; data—unlocks unprecedented generalization. This approach enables robust inference on out-of-distribution data, far surpassing traditional methods.&lt;/p></description></item></channel></rss>